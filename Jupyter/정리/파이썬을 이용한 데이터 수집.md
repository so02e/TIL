# 1. 빅데이터 처리를 위한 파이썬

* `파이썬`은 스크립트 형태의 범용 프로그래밍 언어로 <u>프로그래밍적 구현</u>에 적합하고, <u>데이터 수집과 처리에 활용할 수 있는 다양한 라이브러리를 제공한</u>다는 강점을 가지고 있다.

* 또한 **웹 서버 프로그래밍, 데이터 분석, 시스템 자동화, IoT 프로그래밍** 까지 활용분야가 다양하다. 

  (R의 경우에는 데이터 분석 위주의 언어이기 때문에 파이썬에 비해 다양성이 떨어진다고 볼 수 있다.)

* 빅데이터 처리 언어로 많은 커뮤니티가 형성되어 있으며 가독성이 좋고, 간결하며, 스탠다드 라이브러리가 잘 갖추어져 있다.

* 데이터 분석 관련 패키지가 최근 몇 년 사이 눈에 띄게 발전하여, **NumPy, SciPy, Pandas, Matplotlib,Seaborn** 등 데이터 분석 관련 오픈 소스 라이브러리들을 무상으로 사용할 수 있다. 

  

<hr>

# 2. Anaconda

* `Anaconda` 는 세계에서 가장 유명한 파이썬 데이터 과학 플랫폼이다. 모든 데이터 과학 패키지를 쉽게 설치하고 패키지, 종속성 및 환경을 관리할 수 있다. 

* **conda, Python** 및 150개가 넘는 과학 패키지와 그 종속성과 함께 제공되는 파이썬 배포판이며 응용프로그램 conda는 패키지 및 환경 관리자이다.

* 데이터 분석 관련 패키지인 **NumPy, SciPy, Pandas, Matplotlib** 등을 포함하고 있다.  

  

<hr>

# 3. 파이썬을 이용한 크롤링

|           정적 웹페이지           |      동적 웹페이지       |
| :-------------------------------: | :----------------------: |
| urllib / requests (웹페이지 요청) | Selenium (웹페이지 요청) |
|     BeautifulSoup (스크래핑)      |                          |



## 1) 정적 크롤링

### (1) urllib 패키지와 웹사이트 요청

`urllib` 패키지는 기본적으로 내장하고 있어 추가 설치가 필요 없다. 

>`urllib.request` : URL 문자열을 가지고 **요청** 기능 제공
>
>`urllib.response` : urllib 모듈에 의해 사용되는 **응답** 기능 관련 클래스들 제공
>
>`urllib.parse` : URL 문자열을 **파싱하여 해석** 하는 기능 제공 (쪼개어 각각을 인식함)
>
>`urllib.error` : urllib.request 에 의해 발생하는 예외 클래스들 제공
>
>`urllib.robotparser` : robots.txt 파일을 구문 분석하는 기능 제공

#### ① urllib.request 모듈

URL 문자열을 가지고 HTTP 요청을 수행한다. 

`urlopen()`함수를 사용하여 웹 서버에 페이지를 요청한다.

```python
res = urllib.request.urlopen("URL 문자열")
```

서버로부터 받은 응답을 저장하여 **응답객체(HTTP Response 객체)**를 반환한다.

※ ==HTTP Response(http.client.HTTP Response) 객체==

웹 서버로부터 받은 응답을 래핑하는 객체로, 응답 헤더나 응답바디의 내용을 추출하는 메서드를 제공한다.

- `HTTPResponse.read()` : 웹 서버가 전달한 **데이터(응답바디)**를 바이트 열로 읽어, 컨텐츠를 받아올 수 있도록 한다. 아규먼트(amt)를 따로 주지 않으면, 전체를 읽어 들인다. 이때 16진수로 이루어진 바이트 열로 읽어 들이기 때문에 **한글을 포함한 HTML의 문서를 읽어야할 때**는 `res.read().decode('utf-8')`을 사용한다. 
- `HTTPResponse.getheaders()`
- `HTTPResponse.msg`
- `HTTPResponse.version`
- `HTTPResponse.status`



※ ==웹 페이지의 인코딩==

웹 페이지가 어떠한 문자 셋으로 작성 되어있는지 파악하는 것이 필요하다 .

①  `<meta>` 태그의 `charset` 정보 확인

② 파이썬 프로그램으로 파악 가능

```python
url = "원하는 URL"
f = urllib.request.urlopen(url)
encoding = f.info().get_content_charset()
```



#### ② urllib.parse모듈

URL 문자열을 **파싱하여 해석**하는 기능이다.

웹 서버 페이지 또는 정보를 요청할 때 함께 전달하는 데이터

`name=value&name=value&name=value&...`

|            GET 방식 요청             |             POST 방식 요청             |
| :----------------------------------: | :------------------------------------: |
| Query 문자열/ URL 문자열과 관련 있음 | 요청 파라미터 / URL 문자열과 관련 없음 |

영문과 숫자는 그대로 전달되지만, 한글은 `%`기호와 함께 16진수 코드 값으로 전달 되어야 한다. 따라서 직접 Query 문자열을 구성해서 전달해야 한다.



##### ⑴ urllib.parse.urlparse("URL 문자열") : 객체 정보 추출

아규먼트에 지정된 URL 문자열의 정보를 다음과 같이 구성하여 저장하는 `urllib.parse.ParseResult` 객체를 리턴

```python
url = urlparse("https://movie.daum.net/moviedb/main?movieId=93252")
```

>:ballot_box_with_check: [리턴된 `urllib.parse.ParsResult` 객체 정보] :ballot_box_with_check:
>
>각 속성(netloc, path, query ...) 들을 이용하여 필요한 정보만을 추출할 수 있다.
>
>```python
>ParseResult(scheme="https", netloc="movie.daum.net",
>			path="/moviedb/main", params="",
>			query="movieId=93252", fragmen="")
>```



##### ⑵ urllib.parse.urlencode() : 쿼리 문자열 생성

**원하는 url로 만들어주는 함수**로 쿼리 문자열을 만들어준다.

메서드의 아규먼트로 지정된 name과 value로 구성된 **딕셔너리 정보**를 정해진 규격의 **Query 문자열 또는 요청 파라미터 문자열로 리턴**함

###### ⓐ GET 방식 요청

- Query 문자열을 포함하여 요청하는 것

- 과정

  1. `urllib.parse.urlencode` 함수로 `name`과 `value`로 구성되는 Query 문자열을 만듦
  2. 이 Query 문자열을 URL 문자열 뒤에 `?%s` 기호 추가하여 요청 URL로 사용

- 예시

  ```
  params = urllib.parse.urlencode({'name': '김혜림', 'age': '25'})
  url = "URL문자열?%s" % params
  urllib.request.urlopen(url)
  ```

###### ⓑ POST 방식 요청

요청 바디 안에 요청 파라미터를 포함하여 요청하는 것

- 과정

  1. GET 방식과 같이 `urllib.parse.urlencode` 함수로 `name`과 `value`로 구성되는 Query 문자열을 만듦

  2. POST 방식 요청에서는 바이트 형식의 문자열로 전달해야 하므로 `encode('ascii')` 메서드를 호출하여 바이트 형식의 문자열로 변경

  3. ```
     urllib.request.urlopen()
     ```

      

     호출 시 바이트 형식의 문자열로 변경된 데이터를 두 번째 아규먼트로 지정!

     - 혹은... `urllib.request.Request` 객체를 지정하는 방법이 있음

- 예시1 - 첫번째 방법

  ```
  data = urllib.parse.urlencode({'name': '김혜림', 'age': '25'})
  postdata = data.encode('ascii')
  url = "URL문자열"
  urllib.request.urlopen(url, postdata)
  ```

- 예시2 - 두번째 방법

  ```
  data = urllib.parse.urlencode({'name': '김혜림', 'age': '25'})
  postdata = data.encode('ascii')
  req = urllib.request.Request(url="URL문자열", data=postdata)
  urllib.request.urlopen(req)
  ```



##### ⑶ urllib.parse.quote("URL 문자열") : 특정 Value 값만 URL 문자열로 변환

특정 Value값만 URL 문자열로 변환하고 싶을 때

`urllib.parse.urlencode` 는 딕셔너리 형태로 Value 입력

- ```
  urllib.parse.quote()
  ```

  - ```
    urllib.parse.quote('가나다ABC123')
    ```

    - `%EA%B0%80%EB%82%98%EB%8B%A4ABC123`

  - ```
    urllib.parse.quote('가나다 ABC123')
    ```

    - `%EA%B0%80%EB%82%98%EB%8B%A4%20ABC123`

- ```
  urllib.parse.quote_plus()
  ```

  - Value값에 공백이 있을 때 이거 사용!

  - ```
    urllib.parse.quote_plus('가나다ABC123')
    ```

    - `%EA%B0%80%EB%82%98%EB%8B%A4ABC123`

  - ```
    urllib.parse.quote_plus('가나다 ABC123')
    ```

    - `%EA%B0%80%EB%82%98%EB%8B%A4+ABC123`

사용 (차이점)

```
query_str1 = "q="+urllib.parse.quote('가나다')
query_str2 = "q="+urllib.parse.quote_plus('가나다')
query_str3 = urllib.parse.urlencode({"q" : "가나다"})

print(query_str1)
print(query_str2)
print(query_str3)
q=%EA%B0%80%EB%82%98%EB%8B%A4
q=%EA%B0%80%EB%82%98%EB%8B%A4
q=%EA%B0%80%EB%82%98%EB%8B%A4
```



- 쿼리문을 포함한 url 전송 후 body 출력

```
import urllib.request
import urllib.parse
params = urllib.parse.urlencode({'category': '역사', 'page':25})
url = "http://unico2013.dothome.co.kr/crawling/exercise.php?%s" % params
r = urllib.request.urlopen(url)
print(r.read().decode('utf-8'))
```









### (2) requests패키지와 웹사이트 요청

> `requests` 패키지를 활용한 웹 페이지 요청
>
> `urllib` 에서 확장?

- HTTP 프로토콜과 관련된 기능 지원
- 파이썬 라이브러리
  - 아나콘다에는 `requests` 패키지가 `site-packages`로 설치되어있음
  - 만일 설치해야한다면 다음 명령으로 설치
    - `conda install requests`
    - `pip install requests`

#### import

```
import requests
```

#### HTTP Response 객체 구성

- Header : 응답상태코드, 응답 메세지
  - name : value로 구성되는 여러 정보들
  - 등등
- Body : HTML 전체 내용
  - `<html>~</html>`

------

## `requests.request()`

- `requests` 패키지의 대표 함수
- HTTP 요청을 서버에 보내고 응답을 받아오는 기능 지원

#### `urllib` 패키지 vs `requests` 패키지

- ```
  urllib
  ```

  - 인코딩하여 바이너리 형태로 데이터 전송
  - 요청 방식 (GET, POST)에 따라서 구현 방법이 달라짐

- ```
  requests
  ```

  - 딕셔너리 형태로 데이터 전송
  - `request()` 함수 호출 시 요청 메소드 (GET, POST)를 명시하여 요청

#### `requests.request(method, url, **kwargs)`

- method
  - 요청 방식 지정
  - GET, POST
    - HTTP Response Header, Body 둘 다 받아옴
  - HEAD
    - HTTP Response Header만 받아옴
  - PUT, DELETE, OPTIONS
- url
  - 요청할 대상 URL 문자열 지정
- params
  - 선택적
  - 요청 시 전달할 Query 문자열 지정
  - 딕셔너리, 튜플리스트, 바이트열 가능
- data
  - 선택적
  - 요청 시 바디에 담아서 전달할 요청 파라미터 지정
  - 딕셔너리, 튜플리스트, 바이트열 가능
- json
  - 선택적
  - 요청 시 바디에 담아서 전달할 JSON 타입의 객체 지정
- auth
  - 선택적
  - 인증처리(로그인)에 사용할 튜플 지정

------

#### 요청 방식 지정

- `requests.request()` 함수에 요청 방식을 지정하여 호출하는 것과 아래의 함수들은 동일

  - `request.get(url, params=None, **kwargs)`
  - `requests.post(url, data=None, json=None, **kwargs)`
  - `requests.head(url, **kwargs)`
  - `requests.put(url, data=None, **kwargs)`
  - `requests.patch(url, data=None, **kwargs)`
  - `requests.delete(url, **kwargs)`

- GET 방식 요청 (2가지 방법)

  ```
  requests.request('GET', url, **kwargs) # 방법1
  requests.get(url, params=None/원하는거, **kwargs) # 방법2
  ```

  - Query 문자열을 포함하여 요청
    - `params 매개변수`에 딕셔너리, 튜플리스트, 바이트열 형식으로 전달
  - Query 문자열을 포함하지 않는 요청
    - `params 매개변수`의 설정 생략

- POST 방식 요청 (2가지 방법)

  ```
  requests.request('POST', url, **kwargs) # 방법1
  requests.post(url, params=None/원하는거, **kwargs) # 방법2
  ```

  - ```
    data 매개변수
    ```

    나

     

    ```
    json 매개변수
    ```

    로 요청 파라미터를 지정하여 요청하는 것이 일반적

    - ```
      data 매개변수
      ```

       

      [선택적]

      - 딕셔너리, 튜플리스트 형식, 바이트열 형식으로 지정

    - ```
      json 매개변수
      ```

       

      [선택적]

      - JSON 객체 형식 지정

------

#### 리턴값 : `requests.models.Response 객체`

- `requests.request()`, `requests.get()`,`requests.head()`, `requests.post()` 함수의 리턴값

- Text

  - **문자열 형식**으로 응답 콘텐츠 추출

  - 추출 시 사용되는 문자 셋은 `ISO-8859-1` 이므로

  - `utf-8`이나 `euc-kr` 문자 셋으로 작성된 콘텐츠 추출 시 한글이 꺠지는 현상 발생

  - 추출 전 응답되는 콘텐츠의 문자 셋 정보를 읽고 `r.encoding='utf-8'`와 같이 설정한 후 추출

    - `r` 은 리턴값을 담은 변수

  - 사용

    ```
    r = requests.request('get', 'URL')
    r.encoding = 'utf-8'
    print(r.text)
    ```

- Content

  - **바이트열 형식** 으로 응답 콘텐츠 추출

  - 응답 콘텐츠가 이미지와 같은 **바이너리 형식**인 경우 사용

  - 한글이 들어간 문자열 형식인 경우 `r.content.decode('utf-8')`를 사용해서 디코드 해야 함)

  - 사용

    ```
    r = requests.request('get', 'URL')
    print(r.content.decode('utf-8'))
    print(r.content)) # 한글 깨짐
    ```

- url

  ```
  r = requests.request('get', 'URL')
  print(r.url) # URL
  ```

------

### 이미지, 바이너리 저장

```
import requests
from PIL import Image
from io import BytesIO

r = requests.get('http://unico2013.dothome.co.kr/image/flower.jpg')
i = Image.open(BytesIO(r.content))
i.save("c:/Temp/test.jpg")
```

------

## 예시

- 쿼리문을 포함한 url 전송 후 body 출력 (1)

```
import requests
dicdata = {'category': '여행', 'page':100}
url = "http://unico2013.dothome.co.kr/crawling/exercise.php"
r = requests.request('GET', url, params=dicdata)
r.encoding = 'utf-8'
print(r.text)
```

쿼리문을 포함한 url 전송 후 body 출력 (2)

```
import requests
dicdata = {'category': '여행', 'page':100}
url = "http://unico2013.dothome.co.kr/crawling/exercise.php"
r = requests.get(url, params=dicdata)
r.encoding = 'utf-8'
print(r.text)
```









### (3) Beautiful Soup패키지

> HTML 및 XML 파일에서 데이터를 추출하기 위한 파이썬 라이브러리
>
> urllib나 request 패키지를 통해 얻은 HTML을 분석하는 라이브러리

- 파이썬에서 기본적으로 제공하는 라이브러리 X

- Anaconda에서 BeautifulSoup 패키지가 Site-Packages로 설치되어있음

  - Anaconda prompt창에서 설치

    ```
    conda install bs4 #BeautifulSoup
    conda install lxml
    conda install html5lib
    ```

- 파서

  - 구문 분석기
  - HTML 및 XML 파일의 내용을 읽을 때 사용

#### 임포트

```
import requests
from bs4 import BeautifulSoup
```

#### 파서 종류

- `html.parser`
- `lxml`
- `lxml-xml`
- `html5lib`
- 파이썬이 내장하고 있는 파서 사용해도 되고, 좀 더 성능이 좋은 파서를 추가로 설치해서 사용해도 됨

------

## HTML 파싱

1. BeautifulSoup의 메인 API인 bs4모듈에서 `BeautifulSoup()` 함수 임포트
2. 파싱할 HTML 문서와 파싱에 사용할 파서를 지정하여 호출하면, `bs4.BeautifulSoup` 객체 리턴
3. HTML 문서에 대한 파싱이 끝나면 트리 구조 형식으로 DOM 객체들이 생성되며, `bs4.BeautifulSoup` 객체를 통해 접근 가능

```
from bs4 import BeautifulSoup
bs = BeautifulSoup(html_doc, 'html.parser')
bs = BeautifulSoup(html_doc, 'lxml')
bs = BeautifulSoup(html_doc, 'lxml.xml')
bs = BeautifulSoup(html_doc, 'html5lib')
```

- ```
  bs
  ```

  가 아닌

   

  ```
  bs.pretify()
  ```

   

  로 출력하면 들여쓰기 적용돼서

  - 문서 내용을 이쁘게 출력함

------

## `bs4.BeautifulSoup` 객체의 태그 접근 방법

- HTML 문서를 파싱하고 `bs4.BeautifulSoup` 객체 생성

  ```
  bs = Beautifulsoup(html_doc, 'html.parser')
  ```

- `<html>`, `<head>`, `<body>` 태그는 제외하고 접근하려는 태그에 **계층 구조**를 적용하여 태그명을 `.` 연산자와 함께 사용

  ```
  bs.태그명
  bs.태그명.태그명
  bs.태그명.태그명.태그명
  ```

- 메서드 사용

  ```
  bs.태그명.find_all()
  bs.find_all()
  ```

- 혹은... `bs.select()`

------

## 트리구조로 태그 접근

#### 태그명 추출

```
bs.태그명.name
```

#### 속성 추출

```
bs.태그명['속성명']
bs.태그명.attrs
```

#### 콘텐츠 추출

```
bs.태그명.string # 자손 태그는 안꺼냄
bs.태그명.text # 자손 태그들 것 까지 모두 꺼내줌
bs.태그명.contents # 리스트 형태로 반환
bs.태그명.strings
bs.태그명.stripped_strings
bs.태그명.string.strip() # 공백 제거
bs.태그명.text.strip()
bs.태그명.get_text()
bs.태그명.get_text(strip=True)
```

#### 부모 태그 추출

```
bs.태그명.parent
```

#### 자식 태그들 추출

```
bs.태그명.children
```

#### 형재 태그 추출

```
bs.태그명.next_sibling
bs.태그명.previous_sibling
bs.태그명.next_siblings
bs.태그명.previous_siblings
```

#### 자손 태그들 추출

```
bs.태그명.descendants
```

------

## 메서드

#### 주요 메서드

- `find_all()`
- `find()`
- `select()`

#### 태그 찾기 기능의 기타 메서드

- `find_parents()` / `find_parent()`
- `find_next_siblings()` / `find_next_sibling()`
- `find_previous_siblings()` / `find_previous_sibling`
- `find_all_next()` / `find_next()`
- `first_all_previous()` / `first_previous()`

#### `bs.find_all()`

```
find_all(name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)
# name : 태그명, 정규표현식을 적용한 태그명, 태그명 리스트, 속성 정보, 함수, 논리값
# attrs : 딕셔너리 형태로 넘기게 해줌
# recursive : 자손 찾을지 말지. True(기본값) 찾기, False 찾지 않기
# text
# limit : 찾을 갯수 제한
```

- 주어진 기준에 맞는 모든 태그들을 찾아오며 결과는 `bs4.element.ResultSet` 객체로 리턴

- 정규 표현식 사용하고 싶으면

- `import re`

  - `re.compile()`의 리턴 값으로 넘겨줘야 함

- 예시

  ```
  find_all('div') # div태그를 찿아라
  find_all(['p', 'img']) # p태그, img태그를 찾아라
  find_all(re.compile('^b')) # b로 시작하는 태그를 찾아라
  find_all(True) # 모든 태그를 찾아라
  find_all(id='link2') # id가 link2인 태그를 찾아라
  find_all(id=re.compile("para$")) # id가 para로 끝나는 태그를 찾아라
  find_all(id=True) # id속성이 있는 모든 태그들을 찾아라
  find_all('a', class_='sister') # a태그를 찾고 class속성이 sister인 태그를 찾아라 클래스의 경우 언더바도 써줘야 함.
  find_all(src=re.compile("png$"), id='link1') # id가 link1인 태그를 찾고, src속성이 png로 끝나는 태그를 찾아라 
  ```

  ```
  find_all(attrs={'src': re.compile('png$'), 'id': 'link1'})
  find_all(attrs={'align': 'right', 'class':'c1'})
  
  # 요즘은 text가 아닌 string으로 바뀜. 근데 text로 사용해도 무관
  find_all(text='example') 
  find_all(text=re.compile('example'))
  find_all(text=re.compile('^test'))
  find_all(text=['example', 'test'])
  fild_all('a', text='python')
  
  find_all('a', limit=2) # 앞에서부터 2개 까지만 찾기
  find_all('p', recursive=False) # 자손은 안찾고 싶으면 False. 기본값은 True
  ```

- 뽑은 태그가 여러 개일 때 하나씩 접근하기 (자손의 경우엔 for문 사용 안하고 그냥 출력해도 같이 출력됨)

  ```
  tags = bs.find_all(...)
  for tag in tags:
  	# tag
  ```

#### `bs.find()`

```
find(name=None, attrs={}, attrs={}, recursive=True, text=None, **kwargs)
```

- 주어진 기준에 맞는 태그 한개를 찾아오며 결과는 `bs4.element.Tag` 객체로 리턴, 결과값이 없으면 `None`을 리턴

- `find_all()`에 `limit=1`로 설정한 것과 동일하게 수행

- 예시

  ```
  find('div') == find_all('div', limit=1)
  ```

#### `bs.select()`

```
select(selector, namespaces=None, limit=None, **kwargs)
```

- 주어진 CSS 선택자에 맞는 태그들을 찾아오며 결과는 `list` 객체로 리턴

- CSS 선택자를 적용한 호출

  ```
  select('태그명')
  select('.클래스명')
  select('#아이디명')
  select('태그명.클래스명')
  ```

- HTML 문서의 트리 구조를 적용한 태그 찾기

  ```
  select('상위태그명 > 자식태그명 > 손자태그명')
  select('상위태그명.클래스명 > 자식태그명.클래스명')
  select('상위태그명.클래스명 > 자식태그명')
  select('상위태그명 > 자식태그명.클래스명')
  
  select('#아이디명 > 태그명.클래스명')
  select('태그명[속성]')
  select('태그명[속성=값]')
  select('태그명[속성$=값]') # 값으로 끝나는 속성을 가진
  select('태그명[속성^=값]') # 값으로 시작하는 속성을 가진
  select('태그명:nth-of-type(3)')
  ```

------

## 예제

- 다음 뉴스 랭킹에서 뉴스의 제목, 신문사명 스크래핑(50개)하고, `CSV 파일`로 저장

  ```
  import requests
  from bs4 import BeautifulSoup
  import re
  
  r = requests.get("https://news.daum.net/ranking/popular/")
  r.encoding = "utf-8"
  bs = BeautifulSoup(r.text, "html.parser")
  
  title = bs.select('#mArticle > div.rank_news > ul.list_news2 > li > div.cont_thumb > strong > a')
  comname = bs.select('#mArticle > div.rank_news > ul.list_news2 > li > div.cont_thumb > strong > span')
  
  titleList = []
  comnameList = []
  for dom in title :
       # 콤마가 있으면 구분자로 인식해서 원하는 값이 나오지 않음
      titleList.append(re.sub("[,]", ' ', dom.string))
  for dom in comname :
      comnameList.append(dom.string)
  print(titleList)
  print(comnameList)
      
  # CSV 파일 쓰기
  with open("news.csv", "wt", encoding="utf-8") as f:
      f.write("newstitle,newscomname\n")
      for index in range(len(titleList)):
          f.write(titleList[index]+","+comnameList[index]+"\n")
  ```

- OpenAPI 읽기

  - `xml` 파일 저장
  - `txt` 파일 쓰기 및 저장

  ```
  from bs4 import BeautifulSoup
  import urllib.request as req
  import io
  
  url = "http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=108"
  savename = "C:/Temp/forecast.xml"
  req.urlretrieve(url, savename)
  
  xml = open(savename, "r", encoding="utf-8").read()
  soup = BeautifulSoup(xml, 'html.parser')
  
  info = {}
  for location in soup.find_all("location"):
      loc = location.find('city').string
      min_w = location.find_all('tmn')
      max_w = location.find_all('tmx')
      weather = [a.string+"~"+b.string for a, b in zip(min_w, max_w)]
  
      if not (loc in info):
          info[loc] = []
      for data in weather:
          info[loc].append(data)
  print(info)
  
  with open('C:/Temp/forecast.txt', "wt", encoding="utf-8") as f:
      for loc in sorted(info.keys()):
          f.write(str(loc)+'\n')
          for name in info[loc]:
              f.write('\t'+str(name)+'\n')
  ```

- 브라우저로 접근

  - 왠만하면 사용하지 X
  - 정말 어쩔 수 없을 때.. 상업적 용도가 아닐때 사용

  ```
  # 어떤 웹사이트는 스크래핑 싫어해서 막아놓았을 수 있음
  # 그래서 브라우저 요청처럼 조작하는 것
  import json
  import urllib.request
  
  #User-Agent를 조작하는 경우 
  hdr = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '+ 
          'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}
  
  req = urllib.request.Request('http://unico2013.dothome.co.kr/crawling/header.php', headers = hdr)
  #req = urllib.request.Request('http://unico2013.dothome.co.kr/crawling/header.php')
  data = urllib.request.urlopen(req).read()
  print(data)
  #page = data.decode('utf-8', 'ignore')
  res_content = json.loads(data)
  
  print(res_content["result"])
  ```





## 2) 동적 크롤링 : Selenium

웹 브라우저를 자동화 하는 도구모음으로, 동적 스크래핑에 사용한다. 

- WebDriver 라는 API를 통해 운영체제에 설치된 크롬이나 파이어폭스 등의 브라우저를 기동시키고 웹 페이지를 로드하고 제어
- 브라우저를 직접 동작시켜 JavaScript에 의해 생성되는 콘텐츠와 Ajax 통신 등을 통해 뒤늦게 불려오는 콘텐츠를 처리할 수 있다.

#### WebDriver API

- 간결한 프로그래밍 인터페이스를 제공하도록 설계
- 동적 웹페이지를 보다 잘 지원할 수 있도록 개발
- 목표
  - 최신 고급 웹 응용 프로그램 테스트 문제에 대한 향상된 지원을 제공하는 잘 디자인된 객체지향 API를 제공하는 것
- `Selenium-WebDriver`는 자동화를 위한 각 브라우저의 기본 지원을 사용하여 브라우저를 직접 호출

#### Selenium 설치

- cmd창에서 pip명령 또는 conda 명령을 통해 설치 가능

```
conda install selenium
pip install selenium
```

#### 임포트 패키지

```
from selenium import webdriver
```

------

## 객체 생성 및 페이지 가져오기

#### WebDriver 객체 생성

- 크롬 드라이버를 기반으로`selenium.webdriver.chrome.webdriver.WebDriver` 객체 생성

  ```
  driver = webdriver.Chrome('C:/Temp/chromedriver')
  ```

  - 아규먼트로 `chromedriver.exe` 파일이 존재하는 디렉토리와 파일명에 대한 패스 정보를 지정하면 Selenium에 의해 관리되는 크롬 브라우저가 기동 됨
  - 드라이버 종료
    - `driver.quit()`
    - 수행 후 바로 꺼져버리기 때문에 굳이 사용 안하고 싶으면 사용 안해도 됨

#### 페이지 가져오기

- `selenium.webdriver.chrome.webdriver.WebDriver` 객체의 `get()` 메서드를 사용하여 크롤링하려는 웹 페이지를 제어하는 크롬 브라우저에 로드하고 렌더링

  ```
  driver.get('크롤링하려는웹페이지URL')
  ```

- 경우에 따라서 페이지 로드가 완료되거나 시작되기 전에 WebDriver가 제어권을 반환할 수 있음

- 견고성을 확보하려면 Explicit and Implicit Waits를 사용하여 요소가 페이지에 존재할 때까지 기다려야 함

  ```
  # 방법1
  driver.implicitly_wait(3)
  driver.get('크롤링하려는웹페이지URL')
  ```

  ```
  # 방법2
  driver.get('크롤링하려는웹페이지URL', 5)
  ```

### 요소 찾기

> WebDriver의 요소 찾기는 WebDriver 객체 및 WebElement 객체에서 제공되는 다음 메서드들을 사용

#### 조건에 맞는 요소 한개 찾기 : WebElement 객체 리턴

```
driver.find_element_by_XXX("XXX에 알맞은 식")
```

#### 조건에 맞는 모든 요소 찾기 : list 객체 리턴

```
driver.find_elements_by_XXX("XXX에 알맞은 식")
```

- 태그의 id 속성 값으로 찾기

  ```
  target = driver.find_element_by_id('Id명')
  ```

- 태그의 class 속성 값으로 찾기

  ```
  target = driver.find_element_by_class_name('클래스명')
  ```

- 태그명으로 찾기

  ```
  target = driver.find_element_by_tag_name('h1')
  ```

- 링크 텍스트로 태그 찾기 (`a태그`의 컨텐트로 찾기)

  ```
  target = driver.find_element_by_link_text('파이썬 학습 사이트')
  ```

- 부분 링크 텍스트로 태그 찾기

  ```
  target = driver.find_element_by_partial_link_text('사이트')
  ```

- CSS 선택자로 태그 찾기

  ```
  target = driver.find_element_by_css_selector('CSS선택자')
  ```

- Xpath로 태그 찾기

  ```
  target = driver.find_element_by_xpath('Xpath')
  ```

### 찾은 객체 다루기

- input 태그

  ```
  from selenium.webdriver.common.keys import keys
  target.send_keys('입력할단어')
  target.send_keys(Keys.Enter)
  ```

- 태그명 추출

  ```
  target.tag_name
  ```

- 텍스트 형식의 콘텐츠

  ```
  target.text
  ```

- 속성값

  ```
  target.get_attribute('속성명')
  ```

- 클릭 이벤트

  ```
  import time # 파이썬 time 패키지
  target.click()
  time.sleep(1)
  ```

  - `click()` 이후엔 시간 조금 주기

- txt 파일로 저장

  ```
  wfile = open("원하는이름.txt","w", encoding="utf-8") 
  wfile.writelines(저장할데이터변수) 
  wfile.close()
  ```



예시

- google 입력창에 '파이썬' 입력

  ```
  from selenium import webdriver
  from selenium.webdriver.common.keys import Keys 
  
  driver = webdriver.Chrome('C:/Temp/chromedriver') # 크롬 드랄이버 위치
  print("WebDriver 객체 : ", type(driver))
  
  driver.get('http://www.google.com/ncr') 
  target=driver.find_element_by_css_selector("[name = 'q']")
  print("찾아온 태그 객체 : ", type(target))
  target.send_keys('파이썬')
  target.send_keys(Keys.ENTER)
  ```

- 네이버 호텔 댓글 스크래핑

  ```
  from selenium import webdriver
  import time
  
  driver = webdriver.Chrome('C:/Temp/chromedriver', 3)
  url = "https://hotel.naver.com/hotels/item?hotelId=hotel:Shilla_Stay_Yeoksam&destination_kor=%EC%8B%A0%EB%9D%BC%EC%8A%A4%ED%85%8C%EC%9D%B4%20%EC%97%AD%EC%82%BC&rooms=2"
  driver.get(url)
  time.sleep(5)
  tabButton = driver.find_element_by_css_selector('div.container.ng-scope > div.content > div.hotel_used_review.ng-isolate-scope > ul > li.ng-scope.item_ta > a')
  tabButton.click()
  time.sleep(2)
  reviewList = []
  while True :
      reviews = driver.find_elements_by_css_selector('div.hotel_used_review.ng-isolate-scope > div.review_ta.ng-scope > ul > li > div.review_desc > p')
      for review in reviews :
          reviewList.append(review.text)
      nextButton = driver.find_element_by_css_selector('div.hotel_used_review.ng-isolate-scope > div.review_ta.ng-scope > div.paginate > a.direction.next')
      if nextButton.get_attribute('class') == "direction next disabled" :
          break
      nextButton.click()
      time.sleep(2)
      
  print(len(reviewList))
  wfile = open("C:/KHR/PYDATAexam/output/naverhotel.txt","w", encoding="utf-8") 
  wfile.writelines(reviewList) 
  wfile.close()
  ```

- 스크래핑할 프레임 바꾸고, 스크롤

  ```
  #소스2 : 스크롤
  from selenium import webdriver
  from selenium.webdriver.common.keys import Keys 
  
  driver = webdriver.Chrome('C:/Temp/chromedriver')
  driver.implicitly_wait(3) 
  driver.get('https://map.naver.com/') 
  import time
  time.sleep(2)
  
  target=driver.find_element_by_css_selector(".input_search")
  
  target.send_keys('서울시어린이도서관')
  target.send_keys(Keys.ENTER)
  
  time.sleep(2)
  # iframe 태그를 사용해서 구성된 페이지 (페이지 안에 다른 프레임) 스크래핑 할 때 switch 해야 함
  driver.switch_to.frame("searchIframe")
  while True :
      count = 9
      while True :
          print("스크롤 : " +str(count))
          try :
              driver.execute_script("var su = arguments[0]; var dom=document.querySelectorAll('#_pcmap_list_scroll_container>ul>li')[su]; dom.scrollIntoView();", count)
              time.sleep(2)
          except :        
              break
          count += 10
      names = driver.find_elements_by_css_selector("span._16f7Q")
      addrs = driver.find_elements_by_css_selector("span._3W_ec")
      if len(names) == 0 :
          print("추출되는 도서관이 더 이상 없음")
          break
      for i in range(len(names)) :
          print(names[i].text,addrs[i].text, sep=", ", end="\n")
      print("------------------------------------------------")
      linkurl = '#app-root > div > div > div._2ky45 > a:nth-child(7)'
      try :
          linkNum = driver.find_element_by_css_selector(linkurl)
      except :
          print("더 이상 다음 페이지 없음")
          break
      linkNum.click()  
      time.sleep(5)
  print("스크래핑 종료")
  
  #driver.quit()
  ```







**urlopen("URL 문자열")** : 내부적으로 주어진 문자열을 가지고 서버에 요청을 보내게 된다.  **http response 객체**를 저장하게 됨.





httpresponse의 속성


HTTPResponse.read ([ : 실제 응답 내용을 가져옴

HTTPResponse.readinto (

HTTPResponse.getheader (name, default=None)

HTTPResponse.getheaders

HTTPResponse.msg : 메시지 

HTTPResponse.version : 버전

HTTPResponse.status : 상태

HTTPResponse.reason

HTTPResponse.closed





# 4. Open API 

- API
  - `Application Programming Interface`
  - 특정 프로그램을 만들기 위해 제공되는 모듈 (함수 등)
  - `Client` > `API` > `DB` > `DB Data`

------

#### Open API

- 공개 API

- 누구나 사용할 수 있도록 공개된 API

- 주로

   

  Rest API

   

  기술을 많이 사용

  - 웹에서 사용하는 API

#### Rest API

- `Representational State Transfer API`
- HTTP 프로토콜을 통해서 정보를 제공하는 함수
- 실질적인 API 사용은 **정해진 구조의 URL 문자열 사용**
- 일반적으로 `XML`, `JSON` 형태로 응답 출력

------

## xml, json 형태로 파일 저장

`exam3` 확인

------

## 네이버 Open API 사용하기

#### 네이버 지역 서비스

- 참고 문서

  `https://developers.naver.com/docs/serviceapi/search/local/local.md#%EC%A7%80%EC%97%AD`

- 코드

  ```
  import urllib.request
  from bs4 import BeautifulSoup
  import json
  
  def naver_search(keyword, callType='JSON') :
      # 발급 받은 키
      client_key = '사용하려면 발급 받기'
      client_secret = '사용하려면 발급 받기'
      
      # 요청 url
      base = 'https://openapi.naver.com/v1/search'
      node = '/local'
      parameters = '?query=' + urllib.parse.quote_plus(keyword) + '&display=5'
      
      if callType=='XML' :
          node = node + '.xml'
      elif callType=='JSON' :
          node = node + '.json'
  
      url = base + node + parameters
      
      # 요청 url에 헤더 포함시키기
      request = urllib.request.Request(url)
      request.add_header("X-Naver-Client-Id",client_key)
      request.add_header("X-Naver-Client-Secret",client_secret)
      
      # 요청한 코드 받아서 열기
      response = urllib.request.urlopen(request)
      rescode = response.getcode()
      response_body = response.read()
      
      # 반환된 code가 200이어야 정상 작동된 것
      if rescode != 200 :
          print('오류 코드 : ' + rescode)
          return
      
      if callType=='XML' :
          soup = BeautifulSoup(response_body.decode('utf-8'), 'xml')
          # print(soup) # 구조 확인하기
          print('[짜장면집에 대한 네이버 지역 정보(XML)]')
          index = 1
          for item in soup.find_all('item') :
              title = item.find('title').string
              address = item.find('address').string
              print(f"{index} : {title}, {address}")
              index = index + 1
              
      elif callType=='JSON' :
          dataList = json.loads(response_body)
          # print(dataList) # 구조 확인하기
          print('[쌀국수집에 대한 네이버 지역 정보(JSON)]')
          index = 1
          for item in dataList['items'] :
              title = item['title']
              address = item['address']
              print(f"{index} : {title}, {address}")
              index = index + 1
  ```

- 실행 및 결과 1

  ```
  naver_search('짜장면', 'XML')
  ```

  ```
  [짜장면집에 대한 네이버 지역 정보(XML)]
  1 : 딘타이펑 명동점, 서울특별시 중구 명동1가 59-1
  2 : 몽중헌 페럼타워점, 서울특별시 중구 수하동 66
  3 : 원흥, 서울특별시 중구 다동 92
  4 : 복성각 덕수궁점, 서울특별시 중구 태평로2가 366-1 오천회관빌딩 2층
  5 : 더 플라자 도원, 서울특별시 중구 태평로2가 23
  ```

- 실행 및 결과 2

  ```
  naver_search('쌀국수')
  ```

  ```
  [쌀국수집에 대한 네이버 지역 정보(JSON)]
  1 : 포하노이, 서울특별시 종로구 청진동 70 B1
  2 : 호아빈 서울시청점, 서울특별시 중구 서소문동 14-2
  3 : 미스사이공 명동, 서울특별시 중구 명동2가 54-2
  4 : 반포식스 광화문점, 서울특별시 종로구 신문로1가 239
  5 : 사이공, 서울특별시 종로구 종로1가 24
  ```







