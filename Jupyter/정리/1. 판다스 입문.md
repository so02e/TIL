# 1. 판다스 입문

## 1) 판다스를 배우는 이유

정형 데이터를 다룰 때 2차원의 형식의 데이터 프레임이 필요하다. 파이썬에서도 이러한 형식의 데이터를 효율적으로 다룰 수 있도록 만들어진 패키지가 `판다스`이다.

#### 빅데이터 처리 언어로서 파이썬의 장점

- 이해하기 쉽고 유연한 문법으로 좋은 접근성을 가짐
- 빅데이터 처리 언어로서 많은 커뮤니티가 형성되어 있음
- 가독성이 좋고, 간결하며, 스탠다드 라이브러리가 잘 갖춰져 있음
- 데이터 분석 관련 패키지가 최근 몇 년 사이 눈에 띄게 발전함
  - 데이터 분석 관련 오픈 소스 라이브러리들을 무상 사용 가능
  - `Numpy`, `SciPy`, `Pandas`. `Matplotlib`, `Seaborn` 등

#### Anaconda

- 세계에서 가장 유명한 파이썬 데이터 과학 플랫폼

- 모든 데이터 과학 패키지를 쉽게 설치하고,

- 패키지, 종속성 및 환경을 관리할 수 있음

- 응용 프로그램

- 

- # 아나콘다

  - Anaconda라는 곳에서 만든 파이썬 배포판

  - 수백 개의 파이썬 패키지를 포함함

  - 회사 내에서도 상업용으로 무료로 사용할 수 있다는 장점이 있음

  - 설치

    `https://www.anaconda.com/products/individual`

  ------

  ## Jupyter Lab

  > Jupyter Notebook과 비슷한 개발 환경
  >
  > 브라우저를 통해서 개발 페이지를 제공함

  - Jupyter Notebook은 파일 목록이 안보이지만
  - Jupyter Lab은 볼 수 있어서 더 편리하다.

  #### Jupyter Lab 열기

  1. `Anaconda Prompt` 창 열기

  2. ```
     jupyter lab
     ```

      

     명령 입력

     - 브라우저로 `JupyterLab` 열기

  #### Jupyter Lab 강제 종료

  ```
  ctrl` + `c
  ```

  #### 환경 설정

  > 자동으로 인식하게 되는 현재 디렉토리 위치는 시스템 사용자명 디렉토리이기 때문에 ,
  >
  > 원하는 디렉토리로 변경해주기 위해서는 해당 환경설정을 해줘야 함

  1. `Anaconda Prompt` 창 열기
  2. `jupyter notebook --generate-config` 명령 실행

  - 설정 파일 생성

  1. `C:\Users\H\.jupyter` 경로를 찾아간다.

     - 컴퓨터에 따라 사용자이름(`H`)은 다름

  2. `jupyter_notebook_config.py` 파일을 메모장으로 오픈

  3. `c:/KHR/` 경로에 `PYDATAexam` 폴더 생성

  4. `373`행으로 이동 후 다음과 같이 수정

     - 디렉토리 위치 변경

     ```
     ## The directory to use for notebooks and kernels
     # Default:"
     c.NotebookApp.notebook_dir = 'c:/KHR/PYDATAexam'
     ```

  5. `Anaconda Prompt` 창 열고, `jupyter lab` 명령 실행

     - `serving notebooks from local directory`(디렉토리 위치)가 변경된 것을 확인할 수 있음

  ------

  ## Anaconda에 가상환경 만들기

  > 가상환경 만드는 이유?
  >
  > 1. python 버전 관리와 패키지 충돌 방지
  > 2. 해당 프로젝트에서 필요한 패키지만 설치하기 위해
  >
  > 따라서 프로젝트별로 각각의 가상환경을 만들고 해당 환경에서 개발하는 것이 바람직함.

  - `pydatavenv` 라는 커널 생성하기
  - 해당 이름은 내가 원하는 이름으로 만든거임!

  1. 파이썬 3.8 기반의 가상환경 `pydatavenv`를 생성하는 명령 실행

     ```
     conda create --name pydatavenv python=3.8
     ```

     - 실행중 `Proceed ([y]/n)?` 이 뜨면 `y` 입력

  2. 가상 경이 잘 만들어졌는지 확인

     ```
     conda info --envs
     ```

     - 이렇게 뜨면 성공

       ```
       # conda environments:
       #
       base                  *  C:\Users\H\anaconda3
       pydatavenv               C:\Users\H\anaconda3\envs\pydatavenv
       ```

  3. 가상환경 활성화

     ```
     conda activate pydatavenv
     ```

  4. `ipykerner` 설치

     ```
     conda install ipykernel
     ```

     - 실행중 `Proceed ([y]/n)?` 이 뜨면 `y` 입력

  5. `pyzmq` 삭제 후 재설치

     ```
     pip uninstall pyzmq
     pip install pyzmq
     ```

  6. `pydatavenv` 라는 가상환경을 `jupyter lab`의 커널로 등록

     ```
     python -m ipykernel install --user --name pydatavenv
     ```

     - 커널 : 실행환경

  7. `Anaconda Prompt` 창 한 개 더 오픈

  8. `jupyter lab` 명령 실행

  #### `pydatavenv` 가상환경에 추가 패키지 설치하기

  - `pydatavenv` 가상환경 활성화된 `Anaconda Prompt` 창에서 아래 명령들 하나씩 입력

    - 가상환경 활성화 : `conda activate pydatavenv`

  - 크롤링을 위한 패키지

    ```
    conda install requests
    conda install pillow
    conda install bs4 # beautiful soup. 파이썬 정적 크롤링
    conda install selenium # 파이썬 동적 크롤링
    conda install lxml
    conda install html5lib
    pip install tweepy
    ```

  - 시각화를 위한 패키지

    ```
    conda install pandas
    conda install matplotlib
    conda install seaborn
    pip install folium
    conda install scikit-learn
    conda install xlrd
    conda install -c conda-forge googlemaps
    conda install openpyxl
    ```

    - 패키지 설치 잘 안될 때, `pyzmq` 삭제 후 재설치

      ```
      pip uninstall pyzmq
      pip install pyzmq
      ```

  - `conda list` 로 설치한 패키지 확인 가능

  - 설치가 완료됐으면 `pydatavenv` 비활성화

    - 가상환경 비활성화 : `conda deactivate`

  #### 동적 크롤링을 위한 환경설정

  > 크롬 드라이브 다운로드

  1. 크롬이 최신 버전인지 먼저 확인
  2. `https://sites.google.com/a/chromium.org/chromedriver/` 이동
  3. Downloads - Latest stable release - `ChromeDriver89.0.4389.23` 선택
  4. `chromedriver_win32.zip` 설치
  5. 압축 해제 후 `chromedriver.exe` 파일을 `c:\Temp` 디렉토리에 저장

- 

   

  ```
  conda
  ```

  - 패키지 및 환경 관리자

- 파이썬과 수학, 과학, 데이터 분석 분야에서 필요한 거의 모든 패키지를 포함함

  - `Numpy`, `SciPy`, `Pandas`. `Matplotlib` 등
  - 

## 2) 판다스의 자료구조

### (1) 시리즈(Series)

하나의 행, 열로만 구성되어 있다.

#### ① 생성

내장함수 `Series()`를 이용한다.

딕셔너리 혹은 리스트가 올 수 있다.

리스트가 온 경우에는 무조건 숫자 인덱스가 적용되고, 딕셔너리를 준 경우 키가 인덱스가 된 경우가 된다. 



# 판다스

> Python Data Analysis Library
>
> 데이터 분석을 위한 파이썬 라이브러리

- 목적
  - 형식적으로 서로 다른 여러 가지 유형의 데이터를 공통의 포맷으로 정리하는 것
- 종류
  - **시리즈**와 **데이터프레임**이라는 구조화된 데이터 형식을 제공
  - 특히 데이터프레임은 데이터 분석 실무에서 자주 사용됨

#### 패키지

```
import pandas as pd
```

- 주로 `pd`로 줄여서 씀

#### 시리즈

- 1차원 배열

#### 데이터프레임

- 2차원 배열
- 여러 시리즈들을 묶어놓은 것 (열단위로)

------

## 변수 타입

- `int`

- `float`

- ```
  object
  ```

  - 문자열

- ```
  NaN
  ```

  - 결측치

------

## 산술연산

- 판다스 객체의 산술 연산은 내부적으로 **3단계 프로세스**를 거침

1. 행/열 인덱스를 기준으로 모든 원소를 정렬한다
2. 동일한 위치에 있는 원소끼리 일대일로 대응시킨다
3. 일대일 대응이 되는 원소끼리 연산을 처리한다. 이때 대잉되는 원소가 없으면 `NaN`으로 처리한다





# 시리즈

> 1차원 배열

#### 시리즈 만들기

```
sr = pandas.Series(딕셔너리/리스트/튜플)
```

- 딕셔너리를 시리즈로 변환하는 방법을 많이 사용

  - 딕셔너리와 시리즈의 구조가 비슷하기 떄문

- 딕셔너리

  로 시리즈를 만들면

  - 딕셔너리의 키 → 인덱스
  - 딕셔너리의 값 → 데이터의 값

- 리스트/튜플

  로 시리즈를 만들면

  - 인덱스가 별도로 정의하지 않으면
    - 디폴트로 `0`부터 자동 지정됨
  - 인덱스를 별도로 정의
    - `sr = pd.Series(list, index=['이름', '생년월일', '성별'])`

#### 인덱스 종류 (2가지)

- **정수형 위치 인덱스** (0부터 시작)
  - 접근
    - `[]`
  - 범위로 줬을 때
    - `[start:end]`
    - end 값은 포함 X
  - 예시
    - `[1, 3] `, `[1:3]`
  - `[-1]` : 마지막 인덱스
- **인덱스 이름 / 인덱스 라벨**
  - 접근
    - `['라벨']` / `["라벨"]`
  - 범위로 줬을 때
    - `[start:end]`
    - end 값도 포함 O
  - 예시
    - `['a', 'b']` / `['a':'d']`

#### 속성

- ```
  sr.index
  ```

  - 시리즈의 인덱스 배열 확인

- ```
  sr.values
  ```

  - 시리즈의 데이터 값의 배열 확인

------

## 연산

> 연산자 : `+` `-` `*` `/`

#### 시리즈 vs 숫자

```
sr + 연산자 + 숫자
```

#### 시리즈 vs 시리즈

```
sr1 + 연산자 + sr2
```

- 같은 인덱스를 가진 원소끼리 계산
- 해당 인덱스에 연산 결과를 매칭하여 새 시리즈를 반환
- 두 시리즈의 원소 개수가 다르거나, 시리즈의 크기가 같더라도 인덱스 값이 다른 경우
  - `NaN`으로 처리
- 같은 인덱스가 양쪽에 모두 존재하여 서로 대응되어도, 어느 한 쪽의 데이터 값이 `NaN`인 경우
  - `NaN`으로 처리

#### 연산 메소드 활용

- 객체 사이에 공통 인덱스가 없는 경우 `NaN`으로 반환하는 상황을 피하기 위해 사용
- 연산 메소드에 `fill_value` 옵션 설정
  - `fill_value = NaN일 경우 채울 값`
- 연산 메소드
  - `sr1.add(sr2, fill_value=0)`
  - `sr1.sub(sr2, fill_value=0)`
  - `sr1.mul(sr2, fill_value=0)`
  - `sr1.div(sr2, fill_value=0)`





### (2) 데이터 프레임(DataFrame)





iloc는 엄격하다. 반드시 숫자만 올 수 있다.

loc는 



# 데이터프레임

> 2차원 배열

#### 데이터프레임 만들기

```
df = pandas.DataFrame(딕셔너리)
```

- 딕셔너리

  - 여러 개의 리스트를 원소로 갖는 딕셔너리를 주로 활용

- 딕셔너리의 값 (리스트) → 시리즈로 변환

  - 데이터프레임의 각 열이 됨

- 딕셔너리의 키 → 시리즈의 이름으로 변환

- 데이터프레임의 열 이름이 됨

- 예시

  ```
  import pandas as pd
  dict_data = {'c0':[1,2,3], 'c1':[4,5,6], 'c2':[7,8,9], 'c3':[10, 11, 12]}
  df = pd.Dataframe(dict_data)
  ```

#### 출력

- 예쁘게 출력

  ```
  display(df)
  ```

  - `print(df)` 대신 사용

- 앞에서 부터 n개 출력

  ```
  df.head(n)
  ```

- 뒤에서 부터 n개 출력

  ```
  df.tail(n)
  ```

#### 기본 정보 보기

- 정보 확인

  ```
  df.info()
  ```

- 자료형 확인

  ```
  df.dtypes
  df.열이름.dtypes
  ```

- `(행, 열)` 크기 확인

  ```
  df.shape
  ```

- 각 열이 가지고 있는 원소 갯수 확인

  ```
  df.count()
  ```

- 특정 열이 가지고 있는 고유값마다의 갯수 확인

  ```
  df.열이름.value_counts()
  ```

  - like.. R의 `factor`?
  - 예시 : 직무별 인원수 출력

- 요약 통계량 확인

  ```
  df.describe()
  ```

  - 옵션
    - `include='all'`
    - 

------

## 행 인덱스 / 열 이름

#### 설정

```
pandas.DataFrame(2차원 배열, index=행 인덱스 배열, columns=열 이름 배열)
```

- 2차원 배열 = 리스트 집합

- 유의 사항 : 각 리스트는 행으로 변환됨

- 예시

  ```
  import pandas as pd
  df = pd.DataFrame([[23, '남', '대학생'], [25, '여', '취준생']], index=['정섭', '혜림'], columns=['나이', '성별', '상태'])
  ```

#### 출력

- `df.index`
  - 행 인덱스 출력
- `df.columns`
  - 열 이름 출력

#### 변경

- `df.rename(index={기존인덱스:새인덱스,...})`

  - 행 인덱스 변경

- `df.rename(columns={기존이름:새이름, ...})`

  - 열 이름 변경

- 행 인덱스 또는 열 이름의 일부를 선택하여 변경 가능.

- 단, 새로운 데이터 프레임 객체를 리턴함

  - 원본 객체를 변경하려면

     

    ```
    inplace=True
    ```

     

    옵션 사용

    - `df.rename(columns={'나이':'연령', '남녀':'성별'}, inplace=True)`

------

## 행

#### 행 선택 ★★

- `iloc`
  - **정수형 위치 인덱스** 사용할 때
  - 범위 지정
    - `df.iloc[start:end]`
    - end 제외
  - 예시
    - 1번행 1번열 출력
      - `df.iloc[0,0]`
    - 3번,4번행의 3열 출력
      - `df.iloc[[2,3],2]`
    - 3번,4번행의 3,4열 출력
      - `df.iloc[[2,3],[2,3]]`
    - 행 범위 지정
      - `df.iloc[1:3, 2]`
    - 열 범위 지정
      - `df.iloc[2, 1:3]`
- `loc`
  - **인덱스 이름**을 기준으로 행 선택할 때
  - 범위 지정
    - `df.loc[start:end]`
    - end 포함

#### 행 추가

- ```
  df.loc['새로운 행 이름'] = 데이터 값 (또는 리스트)
  ```

  - 추가하려는 열 이름 대신 **숫자**도 가능

#### 행 삭제

- `df.drop(행인덱스/리스트, axis=0)`
  - 행 삭제
  - 동시에 여러 행 삭제하려면
    - 리스트 형태로 입력
  - 원본 객체 직접 변경
    - `inplace=True` 옵션 추가

------

## 열

#### 열 선택

- `df['열이름']`
- `df[['열이름1', '열이름2', ...]]`
  - 여러 행 지정 선택
- `df.열이름`
  - 열이름이 문자열 형태인 경우만
- `df[::-1]`
  - 역순 출력

#### 열 추가

- ```
  df['추가하려는 열 이름'] = 데이터값
  ```

  - 이 때 모든 행에 데이터값으로 동일한 값이 입력 됨

#### 열 삭제

- ```
  df.drop(열이름/리스트, axis=1)
  ```

  - 열 삭제

  - 동시에 여러 열 삭제하려면

    - 리스트 형태로 입력

  - 원본 객체 직접 변경

    - `inplace=True` 옵션 추가

  - axis 기본값 :

     

    ```
    0
    ```

    - 행 삭제

------

#### 원소 값 변경

- ```
  df.iloc[0,3]=3
  ```

  - `1번째 행`의 `4번째 열` 값을 `80`으로 변경

- ```
  df.loc['혜림','수학'] = 30
  ```

  - `혜림 행`의 `수학 열` 값을 `30`으로 변경

- ```
  df.loc['마이콜', '국어':'수학'] = 100
  ```

  - `마이콜 행`의 `국어, 영어, 수학 열` 값을 모두 `100`으로 변경

- ```
  df.iloc[0,[1,3]] = 50
  ```

  - `0번째 행`의 `2, 4번째 열` 값을 `50`으로 변경

- ```
  df.iloc[0,[1,3]] = 50, 100
  ```

  - `1번째 행`의 `2번째 열` 값은 `50`으로
  - `1번째 행`의 `4번째 열` 값은 `100`으로 변경

- ```
  df.iloc[1, 1:4] = 50
  ```

  - `2번째 행`의 `1~3번째 열` 값을 `50`으로 변경

#### 데이터 프레임 복사

- `df_new = df.copy()`

#### 전치

- ```
  df = df.transpose()
  ```

  - 메소드 활용

- ```
  df = df.T
  ```

  - 클래스 속성 활용

------

## 인덱스 활용

- `df.head(5)`

- `df.tail(5)`

- `df.set_index('열이름')`

  - df의 열 중 하나를 선택하여 행 인덱스로 지정
  - 새로운 데이터프레임 객체 반환
    - `inplace=True` 사용 가능

- `df.reindex(새로운 인덱스 리스트)`

  - 데이터프레임의 행 인덱스를 새로운 배열로 재지정

  - 기존 데이터프레임에 존재하지 않는 행 인덱스가 새롭게 추가되는 경우, 그 행의 데이터값은

     

    ```
    NaN(결측치)
    ```

    로 채워짐

    - `fill_value=0` 사용 가능

  - 새로운 데이터프레임 객체를 반환

    - `inplace=True` 사용 가능

- `df.reset_index()`

  - 행 인덱스 초기화
    - 정수형 위치 인덱스로 초기화
  - 기존행 인덱스는 열로 이동
  - 새로운 데이터프레임 객체를 반환
    - `inplace=True` 사용 가능

------

## 정렬

- `df.sort_index()`

  - 행 인덱스를 기준으로 정렬

  - ```
    ascending
    ```

     

    옵션

    - `True` : 오름차순 (기본값)
    - `False` : 내림차순

  - 새롭게 정렬된 데이터프레임 객체 반환

    - `inplace=True` 사용 가능 : 원본에 적용

- `df.sort_values(by='열이름')`

  - 열 기준으로 정렬

  - ```
    ascending
    ```

     

    옵션

    - `True` : 오름차순 (기본값)
    - `False` : 내림차순

  - 새롭게 정렬된 데이터프레임 객체 반환

    - `inplace=True` 사용 가능 : 원본에 적용

------

## 연산

> 연산자 : `+` `-` `*` `/`
>
> 행/열 인덱스를 기준으로 정렬하고, 일대일 대응되는 원소끼리 연산

#### 데이터프레임 vs 숫자

```
df + 연산자 + 숫자
```

- 모든 원소에 연산을 함
- 기존 데이터프레임의 형태를 그대로 유지한 채, 원소 값만 새로운 값으로 변환됨

#### 데이터프레임 vs 데이터프레임

```
df1 + 연산자 + df2
```

- 각 데이터프레임의 같은 행, 같은 열 위치에 있는 원소끼리 계산

- 동일한 위치의 원소끼리 계산한 결과값을 원래 위치에 다시 입력하여 데이터프레임을 만듬

- 대응되는 것이 없으면 `NaN`으로 채워짐

  - 예시

    - `df1` : 열A, 열B, 열C

    - `df2` : 열A, 열B

    - ```
      df1 + df2
      ```

       

      : 열A, 열B, 열C

      - 열C의 값들은 모두 `NaN`으로 채워짐

#### 연산 메소드 활용

- 객체 사이에 공통 인덱스가 없는 경우 `NaN`으로 반환하는 상황을 피하기 위해 사용
- 연산 메소드에 `fill_value` 옵션 설정
  - `fill_value = NaN일 경우 채울 값`
- 연산 메소드
  - `df1.add(df2, fill_value=0)`
  - `df1.sub(df2, fill_value=0)`
  - `df1.mul(df2, fill_value=0)`
  - `df1.div(df2, fill_value=0)`

------

## 통계 함수 적용

#### 인덱스 마다의 합

```
df.sum()
```

#### 평균값

- 모든 열의 평균값

  ```
  df.mean()
  ```

- 특정 열의 평균값

  ```
  df['열이름'].mean()
  ```

#### 중간값

- 모든 열의 중간값

  ```
  df.median()
  ```

- 특정 열의 중간값

  ```
  df['열이름'].median()
  ```

#### 최대값

- 모든 열의 최대값

  ```
  df.max()
  ```

- 특정 열의 최대값

  ```
  df['열이름'].max()
  ```

#### 최소값

- 모든 열의 최소값

  ```
  df.min()
  ```

- 특정 열의 최소값

  ```
  df['열이름'].min()
  ```

#### 표준편차

- 모든 열의 표준편차

  ```
  df.std()
  ```

- 특정 열의 표준편차

  ```
  df['특정열'].std()
  ```

#### 상관계수

- 모든 열의 상관계수

  ```
  df.corr()
  ```

- 특정 열의 상관계수

  ```
  df[['특정열1', '특정열2']].corr()
  ```

#### 람다 함수 사용

```
df.columns.map(lambda x : str(x)+'년')
```

------

## NaN 값 처리

- 누락값(NaN)을 0으로 채움

  ```
  df.fillna(0, inplace=True)
  ```

- 누락값(NaN)을 앞 데이터로 채움

  ```
  df = df.fillna(method='ffill')
  ```

  - 병합된 부분이어서 NaN으로 값이 들어간 경우

  

## 3) 인덱싱



## 4) 산술연산













# 데이터 입출력

- 판다스는 다양한 형태의 외부 파일을 읽어와서 **데이터프레임을 변환**하는 함수를 제공
- 어떤 파일이든 데이터프레임으로 변환되고 나면, 판다스의 모든 함수와 기능을 자유롭게 사용할 수 있음
- 반대로, 데이터프레임을 다양한 유형의 파일로 저장할 수도 있음

------

## 판다스 데이터 입출력 도구

> Reader : 함수
>
> Writer : 메소드

#### CSV

- Reader : `read_csv`
- Writer : `to_csv`

#### JSON

- Reader : `read_json`
- Writer : `to_json`

#### HTML

- Reader : `read_html`
- Writer : `to_html`

#### Local clipboard

- Reader : `read_clipboard`
- Writer : `to_clipboard`

#### MS Excel

- Reader : `read_excel`
- Writer : `to_excel`

#### HDF5 Format (하둡에 저장되어있는 파일)

- Reader : `read_hdf`
- Writer : `to_hdf`

#### SQL

- Reader : `read_sql`
- Writer : `to_sql`

------

## CSV 파일

> 데이터 값을 쉼표`,` 로 구분하고 있는 텍스트 파일
>
> `,`로 열을 구분하고 `줄바꿈`으로 행을 구분

#### 읽기

```
pd.read_csv("파일경로.csv")
```

- 옵션

  - `path`

  - ```
    sep
    ```

    - 텍스트 데이터를 필드별로 구분하는 문자
    - CSV 파일에 따라서 `,` 대신, 탭`\t` 이나 공백`" "`으로 텍스트를 구분하기도 함

  - ```
    header
    ```

    - 열 이름으로 사용될 행의 번호

    - `0` : 0행을 열로 지정

    - `1` : 1행을 열로 지정

    - ```
      None
      ```

       

      : 행을 열로 지정하지 않음

      - 0부터 열 이름 자동 지정

  - ```
    index_col
    ```

    - 행 인덱스로 사용할 열의 번호 또는 열이름

    - ```
      False
      ```

       

      : 0부터 숫자 인덱스 자동 지정

      - `열이름` : CSV파일이 가지고 있는 해당열을 인덱스로 지정

  - ```
    encoding
    ```

    - 텍스트 인코딩 종류를 지정

#### 저장

```
df.to_csv("파일경로.csv")
```

- 옵션

  - ```
    index=None
    ```

    - 인덱스 없이 저장

  - ```
    header=None
    ```

    - 컬럼 없이 저장

------

## Excel 파일

#### 읽기

```
pd.read_excel("파일경로.xlsx")
```

- header 옵션을 추가하지 않은 경우
  - 첫 행이 열 이름을 구성함

#### 저장

```
df.to_excel("파일경로.xlsx")
```

#### 여러 개의 데이터프레임을 하나의 Excel 파일로 저장

```
writer = pd.ExcelWriter("파일경로.xlsx")
df1.to_excel(writer, sheet_name='sheet1')
df2.to_excel(writer, sheet_name='sheet2')
```

- 하나의 Excel 파일에 여러 시트로 저장
- `ExcelWriter()` 함수
  - Excel 워크북 객체 (Excel 파일) 생성
- `to_excel`
  - 인자
    - Excel 워크북 객체
    - `sheet_name` 옵션

------

## JSON

> JavaScript에서 유래한 데이터 공유를 목적으로 개발된 특수한 파일 형식
>
> 파이썬 딕셔너리와 비슷하게 `Key:Value` 구조를 가짐

#### 읽기

```
pd.read_json("파일경로.json")
```

#### 저장

```
df.to_json("파일경로.json")
```

- 열 이름이 `Key` 가 됨
- 행이름(Key)과 데이터(Value)는 딕셔너리 형태로 `Value`가 됨

------

## HTML

#### 읽기

```
pd.read_html("URL 또는 파일경로.html")
```

- **HTML 웹 페이지에 있는 `<table>` 태그에서 표 형식의 데이터**를 모두 찾아서 데이터 프레임으로 변환
- 한 페이지 내에 **여러 표**가 있는 경우
  - 각각 별도의 데이터 프레임으로 변환 되기 떄문에
  - **여러 개의 데이터프레임을 원소로 갖는 리스트가 반환됨**

#### 저장

```
df.to_html("")
```

------

## API 활용하여 데이터 수집하기





# 넘파이

# numpy

> 수치 데이터를 다루는 데 사용됨
>
> 수치 연산과 관련된 여러 메소드 제공

#### 임포트

```
import numpy as np
```

------

## 메소드

- `np.arange(시작, 끝, step)`

  - 끝은 포함하지 않음
  - like, 파이썬의 `range()`

- `np.random.normal(loc, scale, size)`

  - 난수 생성

  - ```
    loc
    ```

    - 평균

  - ```
    scale
    ```

    - 표준편차

  - ```
    size
    ```

    - 뽑을 갯수







# 데이터 전처리

# 데이터 전처리

#### 목차

- 누락 데이터 처리
- 중복 데이터 처리
- 데이터 표준화
- 범주형(카테고리) 데이터 처리
- 정규화
- 시계열 데이터

------

## 누락 데이터 처리

> 결측치 처리
>
> 대부분 새로운 dataframe을 반환

```
import seaborn as sns
df = sns.load_dataset('titanic')
```

#### 1. 누락 데이터 확인

- 방법1

  ```
  display(df.head())
  ```

- 방법2

  ```
  print(df.info())
  ```

- 방법3

  ```
  nan_deck = df['deck'].value_counts(dropna=False)
  print(nan_deck)
  ```

  - deck라는 열의 NaN 갯수 확인

- 방법4

  ```
  df.isnull() # 누락 데이터면 True, 존재하면 False
  df.notnull() # 누락 데이터면 False, 존재하면 True
  ```

  ```
  df.isnull().sum(axis=0) # sum()은 True인 갯수 반환
  ```

  - 누락 데이터의 갯수 반환

#### 2. 누락 데이터 처리

- **누락 데이터 제거**

  - 누락 데이터가 들어있는 행 제거

    ```
    df = df.dropna(axis=0)
    ```

    - 특정 열로 한정하기

      ```
      df = df.dropna(subset=['열이름'], axis=0)
      ```

  - 누락 데이터가 들어있는 열 제거

    ```
    df = df.dropna(axis=1)
    ```

  - `thresh=갯수` 옵션

    - NaN가 갯수가 이상 있는 행, 열만 삭제함

  - `how='명령'` 옵션

    - `'all'` : 모두 NaN이면 삭제
    - `'any'` : 하나라도 NaN이면 삭제

- **누락 데이터 치환**

  ```
  df['열이름'].fillna(대체값, inplace=True)
  ```

  - `method='명령'` 옵션

    - ```
      'ffill'
      ```

      - NaN이 있는 행의 직전 행에 있는 값으로 치환

    - ```
      'bfill'
      ```

      - NaN이 있는 행의 바로 다음 행에 있는 값으로 치환

  - 예시1 (평균값으로 치환하기)

    ```
    mean_age = df['age'].mean(axis=0)
    df['age'].fillna(mean_age, inplace=True)
    ```

  - 예시2 (최빈값으로 치환하기)

    ```
    most_freq = df['age'].value_counts(dropna=True).idxmax()
    df['age'].fillna(most_freq, inplace=True)
    ```

------

## 중복 데이터 처리

#### 1. 중복 데이터 확인

```
df_dup = df.duplicated()
```

- 전에 나온 행들과 비교하여 중복되는 행이면 `True` 반환, 아니면 False 반환

- 특정 열만 확인하기

  ```
  df_dup = df['열이름'].duplicated()
  ```

#### 2. 중복 데이터 제거

```
df = df.drop_duplicates()
```

- ```
  subset=['열이름1', '열이름2', ...]
  ```

   

  옵션

  - 특정 열만 중복 판별하고 삭제하기

------

## 데이터 변환

#### 단위 환산

- 같은 데이터셋 안에서 서로 다른 측정 단위를 사용한다면, 전체 데이터의 일관성 측면에서 문제가 발생함
- 같은 단위로 변환하자!

#### 자료형 변환

- 데이터의 자료형을 확인하고 적절한 자료형으로 변환하자!

------

## 범주형(카테고리) 데이터 처리

#### 구간 분할

- 데이터 분석 알고리즘에 따라서 연속 데이터를 그대로 사용하기 보다는 일정한 구간으로 나눠서 분석하는 것이 효율적인 경우가 있음
- 구간 분할 : 연속 변수를 일정한 구간으로 나누고, 각 구간을 범주형 이산 변수로 변환하는 과정

```
bin_drivers = np.histogram(df['열이름'], bins=3)
bin_names = ['범주1', '범주2', '범주3'] # 원하는 이름으로 지정
df['새로추가열'] = pd.cut(x=df['열이름'], # 데이터 배열
				   bins=bin_drivers, # 경계값 리스트
				   labels=bin_names, # bin 이름
				   include_lowest=True) # 첫 경계값 포함
```

#### 더미 변수

- 카테고리를 나타내는 범주형 데이터를 회귀분석 등 머신러닝 알고리즘에 바로 사용할 수 없는 경우가 있음

- 그래서 컴퓨터가 인식 가능한 입력값으로 변환해야 함

- 이럴 때 숫자

   

  ```
  0
  ```

   

  또는

   

  ```
  1
  ```

  로 표현되는

   

  ```
  더미 변수
  ```

  를 사용함.

  - `0`과 `1`은 어떤 특성이 있는지 없는지 여부만을 표시함

- ```
  get_dummies()
  ```

   

  함수 사용

  - 범주형 변수의 모든 고유값을 각각 새로운 더미 변수로 변환함
  - 각 더미 변수가 본래 속해 있던 행에는 `1`, 속하지 않았던 다른 행에는 `0`이 입력됨

```
bin_drivers = np.histogram(df['열이름'], bins=3)
bin_names = ['범주1', '범주2', '범주3'] # 원하는 이름으로 지정
df['새로추가열'] = pd.cut(x=df['열이름'], # 데이터 배열
				   bins=bin_drivers, # 경계값 리스트
				   labels=bin_names, # bin 이름
				   include_lowest=True) # 첫 경계값 포함
dummis = pd.gt_dummies(df['새로추가열'])
```

------

## 정규화

- 각 변수에 들어있는 숫자 데이터의 상대적 크기 차이 때문에 머신러닝 분석 결과가 달라질 수 있음
- 따라서 숫자 데이터의 상대적인 크기 차이를 제거할 필요가 있음
- 정규화 : 각 열에 속하는 데이터값을 동일한 크기 기준으로 나눈 비율로 나타내는 것
  - 정규화 과정을 거친 데이터의 범위
    - `0` ~ `1` 또는 `-1 `~ `1`

#### Min-Max scailling

- 각 열의 데이터 중에서 최대값과 최소값을 뺀 값으로 나누는 방법

------

## 시계열 데이터

> 날짜와 시간

- `Timestamp` : 특정한 시점을 기록하는 객체
- `Period` : 두 시점 사이의 일정한 기간을 나타내는 객체

#### 다른 자료형을 시계열 객체로 변환

- 판다스는 다른 자료형으로 저장된 시간 데이터를 판다스 시계열 객체인 `Timestamp`로 변환하는 함수를 제공

- **문자열(object)을 Timestamp로 변환**

  ```
  df['새로운열이름'] = pd.to_datetime(df['변환하고싶은열이름'])
  ```

  - 판다스 Timestamp를 나타내는 `datetime64` 자료형으로 변환됨

  - `format` 옵션

    ```
    pd.to_datetime('2021-01-10 13-30', format='%Y-%m-%d %H-%M')
    ```

    ```
    pd.to_datetime('01102021', format='%m%d%Y')
    ```

  - **DatetimeIndex**

    ```
    df.set_index('새로운열이름', inplace=True)
    ```

    - 시계열 값을 행 인덱스로 지정하면 판다스는 `DatetimeIndex`로 저장함
    - 시간 순서에 맞춰 인덱싱 또는 슬라이싱하기 편리함

- **문자열(object)을 Period로 변환**

#### 시계열 데이터 만들기

#### 시계열 데이터 활용









